{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mini: Lab 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, I have chosen to analyze discussions on True Detective, another anthology series, in this case one whose first season was way more acclaimed than the second one. I am interested in checking if there are more positive adjectives in the reviews of the first season than in the second, due to the difference of reviews mentioned before. Additionally I will also inspect the nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data discussions.p\n",
    "df = pd.read_pickle('discussions.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6cba51ef6db40a98a6ad517fb28fa7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Process the posts\n",
    "posts = df.post.values\n",
    "processed_texts = [text for text in tqdm(nlp.pipe(posts, \n",
    "                                              n_process=-1, \n",
    "                                              disable=[\"ner\",\n",
    "                                                       \"parser\"]),\n",
    "                                          total=len(posts\n",
    "                                                   ))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the processed texts as an attribute of the df\n",
    "df['processed_texts'] = processed_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_detective_posts = df[df.title == \"True Detective\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2015, 2014, 2019])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Three seasons of True Detective have been released: in 2014, 2015 and 2019, respectively\n",
    "true_detective_posts.year.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = true_detective_posts[true_detective_posts.year == 2014]\n",
    "s2 = true_detective_posts[true_detective_posts.year == 2015]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['faerie',\n",
       " 'wing',\n",
       " 'person',\n",
       " 'prostitute',\n",
       " 'customer',\n",
       " 'world',\n",
       " 'end',\n",
       " 'deal',\n",
       " 'deal',\n",
       " 'sense',\n",
       " 'staple',\n",
       " 'trope',\n",
       " 'excuse',\n",
       " 'conceal',\n",
       " 'problem',\n",
       " 'case',\n",
       " 'sense',\n",
       " 'crow',\n",
       " 'mask',\n",
       " 'corner',\n",
       " 'shack',\n",
       " 'location',\n",
       " 'cover',\n",
       " 'cover',\n",
       " 'season',\n",
       " 'moment',\n",
       " 'actress',\n",
       " 'memory',\n",
       " 'picture',\n",
       " 'mayor',\n",
       " 'end',\n",
       " 'suit',\n",
       " 'jacket',\n",
       " 'gunshot',\n",
       " 'wound',\n",
       " 'chest',\n",
       " 'foreshadowing',\n",
       " 'father',\n",
       " 'commune',\n",
       " 'kind',\n",
       " 'role',\n",
       " 'childhood',\n",
       " 'episode',\n",
       " 'kid',\n",
       " 'suicide',\n",
       " 'prison',\n",
       " 'detective',\n",
       " 'father',\n",
       " 'belief',\n",
       " 'lifestyle',\n",
       " 'shit',\n",
       " 'investigation',\n",
       " 'cop',\n",
       " 'fun',\n",
       " 'setup',\n",
       " 'agency',\n",
       " 'mayor',\n",
       " 'office',\n",
       " 'plot',\n",
       " 'shootout',\n",
       " 'ghetto',\n",
       " 'drug',\n",
       " 'idk',\n",
       " 'season',\n",
       " 'complaint',\n",
       " 'incident',\n",
       " 'news',\n",
       " 'woman',\n",
       " 'man',\n",
       " 'thing',\n",
       " 'woman',\n",
       " 'result',\n",
       " 'training',\n",
       " 'point',\n",
       " 'story',\n",
       " 'man',\n",
       " 'stomach',\n",
       " 'power',\n",
       " 'dude',\n",
       " 'tooth',\n",
       " 'way',\n",
       " 'watch',\n",
       " 'thing',\n",
       " 'pawn',\n",
       " 'shop',\n",
       " 'shit',\n",
       " 'politician',\n",
       " 'police',\n",
       " 'dick',\n",
       " 'orgy',\n",
       " 'invite',\n",
       " 'time',\n",
       " 'place',\n",
       " 'buss',\n",
       " 'shotgun',\n",
       " 'torso',\n",
       " 'hope',\n",
       " 'trait',\n",
       " 'show',\n",
       " 'movie',\n",
       " 'book',\n",
       " 'medium',\n",
       " 'touch',\n",
       " 'mom',\n",
       " 'wildling',\n",
       " 'food',\n",
       " 'potato',\n",
       " 'dad',\n",
       " 'wildling',\n",
       " '*',\n",
       " 'product',\n",
       " 'placement',\n",
       " 'movie',\n",
       " 'e',\n",
       " '-',\n",
       " 'cigarette',\n",
       " 'minute',\n",
       " 'coincidence',\n",
       " 'reference',\n",
       " 'testicle',\n",
       " 'thing',\n",
       " 'movie',\n",
       " 'way',\n",
       " 'drug',\n",
       " 'sound',\n",
       " 'ecstasy',\n",
       " 'drug',\n",
       " 'thing',\n",
       " 'halucinagen',\n",
       " 'scene',\n",
       " 'man',\n",
       " 'sex',\n",
       " 'sister',\n",
       " 'walk',\n",
       " 'guy',\n",
       " 'comparison',\n",
       " 'knowledge',\n",
       " 'back',\n",
       " 'head',\n",
       " 'life',\n",
       " 'party',\n",
       " 'cop',\n",
       " 'bus',\n",
       " 'guy',\n",
       " 'sonuvabitch',\n",
       " 'pleasure',\n",
       " 'line',\n",
       " 'detective',\n",
       " 'coil',\n",
       " 'circle',\n",
       " 'thing',\n",
       " 'hippie',\n",
       " 'commune',\n",
       " 'kid',\n",
       " 'child',\n",
       " 'abuse',\n",
       " 'abuse',\n",
       " 'sister',\n",
       " 'porn',\n",
       " 'dad',\n",
       " 'porn',\n",
       " 'stuff',\n",
       " 'site',\n",
       " 'dominatrix',\n",
       " 'bondage',\n",
       " 'way',\n",
       " 'knife',\n",
       " 'man',\n",
       " 'knife',\n",
       " 'gun',\n",
       " 'knife',\n",
       " 'face',\n",
       " 'scene',\n",
       " 'boyfriend',\n",
       " 'guess',\n",
       " 'rape',\n",
       " 'power',\n",
       " 'play',\n",
       " 'fantasy',\n",
       " 'self',\n",
       " 'fantasy',\n",
       " 'bf',\n",
       " 'porn',\n",
       " 'resist',\n",
       " 'cop',\n",
       " 'control',\n",
       " 'people',\n",
       " 'thing',\n",
       " 'boyfriend',\n",
       " 'butt',\n",
       " 'stuff',\n",
       " 'money',\n",
       " 'shock',\n",
       " 'value',\n",
       " 'butt',\n",
       " 'stuff',\n",
       " 'dick',\n",
       " 'season',\n",
       " 'ending',\n",
       " 'season',\n",
       " 'ending',\n",
       " 'outset',\n",
       " 'mind',\n",
       " 'suitcase',\n",
       " 'money',\n",
       " 'money',\n",
       " 'way',\n",
       " 'reason',\n",
       " 'some',\n",
       " 'muscle',\n",
       " 'laugh',\n",
       " 'thank',\n",
       " 'man',\n",
       " 'theme',\n",
       " 'shootout',\n",
       " 'civilian',\n",
       " 'situation',\n",
       " 'shit',\n",
       " 'police',\n",
       " 'shooting',\n",
       " 'case',\n",
       " 'one',\n",
       " 'ahh',\n",
       " 'line',\n",
       " 'hydraulic',\n",
       " 'detective',\n",
       " 'tooth',\n",
       " 'reincarnation',\n",
       " 'portrayal',\n",
       " 'kind',\n",
       " 'shit',\n",
       " 'life',\n",
       " 'comment',\n",
       " 'season',\n",
       " 'corruption',\n",
       " 'crime',\n",
       " 'cartel',\n",
       " 'politician',\n",
       " 'son',\n",
       " 'son',\n",
       " 'baby',\n",
       " 'people',\n",
       " 'orgy',\n",
       " 'truth',\n",
       " 'drug',\n",
       " 'dna',\n",
       " 'wallet',\n",
       " 'guy',\n",
       " 'message',\n",
       " 'face',\n",
       " 'none',\n",
       " 'couple',\n",
       " 'episode',\n",
       " 'thank',\n",
       " 'line',\n",
       " 'resolution',\n",
       " 'episode',\n",
       " 'climax',\n",
       " 'viewer',\n",
       " 'moment',\n",
       " 'beginning',\n",
       " 'season',\n",
       " 'act',\n",
       " 'shit',\n",
       " 'thank',\n",
       " 'connection',\n",
       " 'one',\n",
       " 'result',\n",
       " 'cop',\n",
       " 'one',\n",
       " 'man',\n",
       " 'waitress',\n",
       " 'nail',\n",
       " 'blake',\n",
       " 'ray',\n",
       " 'frank',\n",
       " 'people',\n",
       " 'life',\n",
       " 'criminal',\n",
       " 'thing',\n",
       " 'rest',\n",
       " 'decency',\n",
       " 'edit',\n",
       " 'character',\n",
       " 'term',\n",
       " 'decency',\n",
       " 'person',\n",
       " 'beginning',\n",
       " 'tooth',\n",
       " 'goon',\n",
       " 'closing',\n",
       " 'song',\n",
       " 'city',\n",
       " 'spoiler',\n",
       " 'picture',\n",
       " 'week',\n",
       " 'thing',\n",
       " 'pistol',\n",
       " 'gun',\n",
       " 'matter',\n",
       " 'detective',\n",
       " 'guy',\n",
       " 'weapon',\n",
       " 'pistol',\n",
       " 'video',\n",
       " 'game',\n",
       " 'bullet',\n",
       " 'father',\n",
       " 'mother',\n",
       " 'kid',\n",
       " 'woo',\n",
       " 'hoo',\n",
       " 'nihilism',\n",
       " 'show',\n",
       " 'other',\n",
       " 'bow',\n",
       " 'top',\n",
       " 'ending',\n",
       " 'ending',\n",
       " 'season',\n",
       " 'whole',\n",
       " 'lead',\n",
       " 'way',\n",
       " 'ending',\n",
       " 'weight',\n",
       " 'way',\n",
       " 'corner',\n",
       " 'world',\n",
       " 'season',\n",
       " 'theme',\n",
       " 'suffering',\n",
       " 'way',\n",
       " 'ticket',\n",
       " 'season',\n",
       " 'lack',\n",
       " 'character',\n",
       " 'development',\n",
       " 'season',\n",
       " 'flaw',\n",
       " 'character',\n",
       " 'transformation',\n",
       " 'redemption',\n",
       " 'part',\n",
       " 'season',\n",
       " 'end',\n",
       " 'stooge',\n",
       " 'scene',\n",
       " 'character',\n",
       " 'catch',\n",
       " 'lot',\n",
       " 'talk',\n",
       " 'video',\n",
       " 'camera',\n",
       " 'drive',\n",
       " 'velcoro',\n",
       " 'guy',\n",
       " 'shit',\n",
       " 'chick',\n",
       " 'sex',\n",
       " 'ring',\n",
       " 'girl',\n",
       " 'eviction',\n",
       " 'scene',\n",
       " 'hell',\n",
       " 'sister',\n",
       " 'opposite',\n",
       " 'episode',\n",
       " 'dream',\n",
       " 'crap',\n",
       " 'did?](https://33.media.tumblr.com/8032b1f4910dbb950a97e28ce250706b',\n",
       " 'tumblr_mn4bl6vtsd1sqn8o2o1_r1_400.gif',\n",
       " 'life',\n",
       " 'time',\n",
       " 'edit',\n",
       " 'name',\n",
       " 'song',\n",
       " 'watch?v',\n",
       " 'opera',\n",
       " 'episode',\n",
       " 'motivation',\n",
       " 'outside',\n",
       " 'dude',\n",
       " 'habit',\n",
       " 'ex',\n",
       " '-',\n",
       " 'wife',\n",
       " 'son',\n",
       " 'object',\n",
       " 'dude',\n",
       " 'thing',\n",
       " 'people',\n",
       " 'episode',\n",
       " 'thing',\n",
       " 'scene',\n",
       " 'coincidence',\n",
       " 'mansion',\n",
       " 'sequence',\n",
       " 'tonight',\n",
       " 'shootout',\n",
       " 'book',\n",
       " 'week',\n",
       " 'rape',\n",
       " 'cheer',\n",
       " 'sense',\n",
       " 'brutality',\n",
       " 'music',\n",
       " 'death',\n",
       " 'man',\n",
       " 'world',\n",
       " 'guy',\n",
       " 'photo',\n",
       " 'motivation',\n",
       " 'audience',\n",
       " 'romance',\n",
       " 'threat',\n",
       " 'rest',\n",
       " 'season',\n",
       " 'payoff',\n",
       " 'diamond',\n",
       " 'parent',\n",
       " 'mother',\n",
       " 'reason',\n",
       " 'kid',\n",
       " 'kid',\n",
       " 'point',\n",
       " 'diamond',\n",
       " 'money',\n",
       " 'echelon',\n",
       " 'society',\n",
       " 'point',\n",
       " 'skyline',\n",
       " 'shot',\n",
       " 'frank',\n",
       " 'tape',\n",
       " 'mobster',\n",
       " 'side',\n",
       " 'track',\n",
       " 'fun',\n",
       " 'episode',\n",
       " 'hero',\n",
       " 'end',\n",
       " 'hand',\n",
       " 'drug',\n",
       " 'dealer',\n",
       " 'guy',\n",
       " 'bakery',\n",
       " 'limbo',\n",
       " 'death',\n",
       " 'message',\n",
       " 'progress',\n",
       " 'corruption',\n",
       " 'back',\n",
       " 'blood',\n",
       " 'other',\n",
       " 'figure',\n",
       " 'conflict',\n",
       " 'season',\n",
       " 'world',\n",
       " 'tv',\n",
       " 'show',\n",
       " 'target',\n",
       " 'bullseye',\n",
       " 'back',\n",
       " 'writer',\n",
       " 'journalist',\n",
       " 'end',\n",
       " 'story',\n",
       " 'rest',\n",
       " 'acting',\n",
       " 'aura',\n",
       " 'rage',\n",
       " 'represent',\n",
       " 'reporter',\n",
       " 'gunfight',\n",
       " 'history',\n",
       " 'nail',\n",
       " 'finale',\n",
       " 'son',\n",
       " 'way',\n",
       " 'friend',\n",
       " 'highlight',\n",
       " 'hair',\n",
       " 'thing',\n",
       " 'deal',\n",
       " 'ex',\n",
       " 'hand',\n",
       " 'trembling',\n",
       " 'order',\n",
       " 'couple',\n",
       " 'situation',\n",
       " 'kind',\n",
       " 'person',\n",
       " 'lot',\n",
       " 'line',\n",
       " 'matter',\n",
       " 'opinion',\n",
       " 'season',\n",
       " 'ending',\n",
       " 'one',\n",
       " 'writer',\n",
       " 'knife',\n",
       " 'season',\n",
       " 'writing',\n",
       " 'line',\n",
       " 'milimeter',\n",
       " 'wall',\n",
       " 'reviewer',\n",
       " 'mouth',\n",
       " 'ending',\n",
       " 'nature',\n",
       " 'noir',\n",
       " 'hip',\n",
       " 'kind',\n",
       " 'dread',\n",
       " 'decay',\n",
       " 'corruption',\n",
       " 'people',\n",
       " 'presumption',\n",
       " 'guilt',\n",
       " 'man',\n",
       " 'secret',\n",
       " 'way',\n",
       " 'whole',\n",
       " 'wife',\n",
       " 'rapist',\n",
       " 'day',\n",
       " 'part',\n",
       " 'investigation',\n",
       " 'upvote',\n",
       " 'assurance',\n",
       " 'universe',\n",
       " 'kind',\n",
       " 'dimension',\n",
       " 'thing',\n",
       " 'girlfriend',\n",
       " 'moment',\n",
       " 'thing',\n",
       " 'dream',\n",
       " 'ball',\n",
       " 'people',\n",
       " 'hope',\n",
       " 'character',\n",
       " 'one',\n",
       " 'bit',\n",
       " 'bird(wo)man',\n",
       " 'mustache',\n",
       " 'episode',\n",
       " 'sub',\n",
       " 'season',\n",
       " 'lot',\n",
       " 'year',\n",
       " 'show',\n",
       " 'people',\n",
       " 'theory',\n",
       " 'apology',\n",
       " 'decor',\n",
       " 'water',\n",
       " 'wallpaper',\n",
       " 'eye',\n",
       " 'person',\n",
       " 'guy',\n",
       " 'cowboy',\n",
       " 'hat',\n",
       " 'table',\n",
       " 'way',\n",
       " 'contract',\n",
       " 'paper',\n",
       " 'trail',\n",
       " 'order',\n",
       " 'lady',\n",
       " 'girl',\n",
       " 'party',\n",
       " 'chance',\n",
       " 'info',\n",
       " 'holloway',\n",
       " 'train',\n",
       " 'station',\n",
       " '#',\n",
       " 'diamond',\n",
       " 'country',\n",
       " 'butt',\n",
       " 'just',\n",
       " 'sub',\n",
       " '-',\n",
       " 'plot',\n",
       " 'distraction',\n",
       " 'audio',\n",
       " 'day',\n",
       " 'blend',\n",
       " 'brow',\n",
       " 'gangster',\n",
       " 'talk',\n",
       " 'majority',\n",
       " 'season',\n",
       " 'person',\n",
       " 'comment',\n",
       " 'foreshadowing',\n",
       " 'death',\n",
       " 'death',\n",
       " 'end',\n",
       " 'episode',\n",
       " 'beginning',\n",
       " 'episode',\n",
       " 'rebirth',\n",
       " 'beginning',\n",
       " 'redemption',\n",
       " 'fate',\n",
       " 'ex',\n",
       " '-',\n",
       " 'wife',\n",
       " 'assaulter',\n",
       " 'man',\n",
       " 'police',\n",
       " 'thing',\n",
       " 'character',\n",
       " 'address',\n",
       " 'call',\n",
       " 'girl',\n",
       " 'club',\n",
       " 'character',\n",
       " 'bar',\n",
       " 'address',\n",
       " 'murder',\n",
       " 'victim',\n",
       " 'house',\n",
       " 'thing',\n",
       " 'girl',\n",
       " 'half',\n",
       " 'half',\n",
       " 'jabawokee',\n",
       " 'hooker',\n",
       " 'security',\n",
       " 'caliber',\n",
       " 'man',\n",
       " 'side',\n",
       " 'effect',\n",
       " 'drug',\n",
       " 'line',\n",
       " 'episode',\n",
       " 'option',\n",
       " 'writer',\n",
       " 'behavior',\n",
       " 'episode',\n",
       " 'keistering',\n",
       " 'people',\n",
       " 'noir',\n",
       " 'noir',\n",
       " 'excuse',\n",
       " 'writing',\n",
       " 'fun',\n",
       " 'fact',\n",
       " 'friend',\n",
       " 'mine',\n",
       " 'song',\n",
       " 'member',\n",
       " 'band',\n",
       " 'message',\n",
       " 'time',\n",
       " 'character',\n",
       " 'brother',\n",
       " 'show',\n",
       " 'moment',\n",
       " 'couple',\n",
       " 'episode',\n",
       " 'show',\n",
       " 'perspective',\n",
       " 'monologue',\n",
       " 'basement',\n",
       " 'writing',\n",
       " 'moment',\n",
       " 'grin',\n",
       " 'face',\n",
       " 'pain',\n",
       " 'story',\n",
       " 'arc',\n",
       " 'protagonist',\n",
       " 'world',\n",
       " 'battle',\n",
       " 'nations',\n",
       " 'soul',\n",
       " 'cult',\n",
       " 'mexicos',\n",
       " 'drug',\n",
       " 'cartel',\n",
       " 'gruesome',\n",
       " 'consequence',\n",
       " 'story',\n",
       " 'cult',\n",
       " 'cartel',\n",
       " 'house',\n",
       " 'mindblown',\n",
       " 'critic',\n",
       " 'season',\n",
       " 'fan',\n",
       " 'season',\n",
       " 'season',\n",
       " 'episode',\n",
       " 'episode',\n",
       " 'season',\n",
       " 'beginning',\n",
       " 'episode',\n",
       " 'ceiling',\n",
       " 'imagery',\n",
       " 'discussion',\n",
       " 'ending',\n",
       " 'tonight',\n",
       " 'episode',\n",
       " 'vibe',\n",
       " 'bit',\n",
       " 'question',\n",
       " 'trailer',\n",
       " 'meeting',\n",
       " 'bar',\n",
       " 'knuckle',\n",
       " 'knuckle',\n",
       " 'bar',\n",
       " 'way',\n",
       " 'killer',\n",
       " 'cinematography',\n",
       " 'design',\n",
       " 'season',\n",
       " 'call',\n",
       " 'fat',\n",
       " 'ginger',\n",
       " 'kid',\n",
       " 'coked',\n",
       " 'photo',\n",
       " 'pace',\n",
       " 'season',\n",
       " 'ride',\n",
       " 'plenty',\n",
       " 'girth',\n",
       " 'ecig',\n",
       " 'cameo',\n",
       " 'time',\n",
       " 'ecig',\n",
       " 'tv',\n",
       " 'tv',\n",
       " 'marker',\n",
       " 'season',\n",
       " 'lot',\n",
       " 'bit',\n",
       " 'season',\n",
       " 'way',\n",
       " 'season',\n",
       " 'part',\n",
       " 'dynamic',\n",
       " 'fact',\n",
       " 'man',\n",
       " 'audience',\n",
       " 'lot',\n",
       " 'problem',\n",
       " 'story',\n",
       " 'look',\n",
       " 'situation',\n",
       " 'dialogue',\n",
       " 'scene',\n",
       " 'driving',\n",
       " 'backseat',\n",
       " 'car',\n",
       " 'character',\n",
       " 'unravelling',\n",
       " 'complexity',\n",
       " 'amount',\n",
       " 'problem',\n",
       " 'one',\n",
       " 'show',\n",
       " 'rest',\n",
       " 'season',\n",
       " 'blame',\n",
       " 'reason',\n",
       " 'fiasco',\n",
       " 'shit',\n",
       " 'outlet',\n",
       " 'party',\n",
       " 'edge',\n",
       " 'plot',\n",
       " 'metal',\n",
       " 'company',\n",
       " 'rail',\n",
       " 'land',\n",
       " 'value',\n",
       " 'foresight',\n",
       " 'rail',\n",
       " 'link',\n",
       " 'company',\n",
       " 'guy',\n",
       " 'hill',\n",
       " 'implication',\n",
       " 'murder',\n",
       " 'money',\n",
       " 'land',\n",
       " 'consortium',\n",
       " 'money',\n",
       " 'drive',\n",
       " 'video',\n",
       " 'chairman',\n",
       " 'psychologist',\n",
       " 'plastic',\n",
       " 'surgeon',\n",
       " 'stuff',\n",
       " 'hand',\n",
       " 'rail',\n",
       " 'link',\n",
       " 'corruption',\n",
       " 'psych',\n",
       " 'hooker',\n",
       " 'party',\n",
       " 'girl',\n",
       " 'people',\n",
       " 'state',\n",
       " 'senator',\n",
       " 'son',\n",
       " 'girl',\n",
       " 'blackmail',\n",
       " 'people',\n",
       " 'interest',\n",
       " 'poker',\n",
       " 'room',\n",
       " 'profit',\n",
       " 'rail',\n",
       " 'land',\n",
       " 'house',\n",
       " 'drive',\n",
       " 'part',\n",
       " 'crow',\n",
       " 'cult',\n",
       " 'thing',\n",
       " 'psychologist',\n",
       " 'part',\n",
       " 'photo',\n",
       " 'kid',\n",
       " 'psychologist',\n",
       " 'wife',\n",
       " 'time',\n",
       " 'tradition',\n",
       " 'patriarch',\n",
       " 'hooker',\n",
       " 'orgy',\n",
       " 'sex',\n",
       " 'shit',\n",
       " 'father',\n",
       " 'age',\n",
       " 'cult',\n",
       " 'girl',\n",
       " 'pole',\n",
       " 'casper',\n",
       " 'daughter',\n",
       " 'hooker',\n",
       " 'relationship',\n",
       " 'scene',\n",
       " 'lot',\n",
       " 'control',\n",
       " 'task',\n",
       " 'pit',\n",
       " 'bull',\n",
       " 'end',\n",
       " 'occult',\n",
       " 'bird',\n",
       " 'cop',\n",
       " 'dept',\n",
       " 'guy',\n",
       " 'mom',\n",
       " 'child',\n",
       " 'vibe',\n",
       " 'people',\n",
       " 'episode',\n",
       " 'viagra',\n",
       " 'bathroom',\n",
       " 'sex',\n",
       " 'girlfriend',\n",
       " 'character',\n",
       " 'edit',\n",
       " 'rock',\n",
       " 'ride',\n",
       " 'middle',\n",
       " 'desert',\n",
       " 'episode',\n",
       " 'flashback',\n",
       " 'viewer',\n",
       " 'character',\n",
       " 'scenery',\n",
       " 'body',\n",
       " 'heat',\n",
       " 'mystery',\n",
       " 'touch',\n",
       " 'evil',\n",
       " 'movie',\n",
       " 'kind',\n",
       " 'delivery',\n",
       " 'style',\n",
       " 'tone',\n",
       " 'present',\n",
       " 'character',\n",
       " 'wtf',\n",
       " 'show',\n",
       " 'friend',\n",
       " 'life',\n",
       " 'humorless',\n",
       " 'genetic',\n",
       " 'people',\n",
       " 'way',\n",
       " 'gene',\n",
       " 'time',\n",
       " 'time',\n",
       " 'search',\n",
       " 'note',\n",
       " 'time',\n",
       " 'resident',\n",
       " 'county',\n",
       " 'independence',\n",
       " 'hell',\n",
       " 'place',\n",
       " 'people',\n",
       " 'drunk',\n",
       " 'payment',\n",
       " 'catalyst',\n",
       " 'car',\n",
       " 'vince',\n",
       " 'company',\n",
       " 'casper',\n",
       " 'share',\n",
       " 'parcel',\n",
       " 'sweat',\n",
       " 'shop',\n",
       " 'shit',\n",
       " 'guy',\n",
       " 'running',\n",
       " 'book',\n",
       " 'competition',\n",
       " 'favor',\n",
       " 'poker',\n",
       " 'room',\n",
       " 'kickback',\n",
       " 'fat',\n",
       " 'date',\n",
       " 'mom',\n",
       " 'person',\n",
       " 'son',\n",
       " 'line',\n",
       " 'guy',\n",
       " 'character',\n",
       " 'city',\n",
       " 'anger',\n",
       " 'world',\n",
       " 'applie',\n",
       " 'sweatshop',\n",
       " 'economic',\n",
       " 'unfeeling',\n",
       " 'capitalist',\n",
       " 'head',\n",
       " 'judge',\n",
       " 'shithead',\n",
       " 'end',\n",
       " 'season',\n",
       " 'lot',\n",
       " 'money',\n",
       " 'corridor',\n",
       " 'deal',\n",
       " 'probe',\n",
       " 'governor',\n",
       " 'office',\n",
       " 'history',\n",
       " 'governor',\n",
       " 'family',\n",
       " 'thing',\n",
       " 'money',\n",
       " 'cop',\n",
       " 'routine',\n",
       " 'man',\n",
       " 'position',\n",
       " 'ambition',\n",
       " 'family',\n",
       " 'woman',\n",
       " 'match',\n",
       " 'justice',\n",
       " 'justice',\n",
       " 'justice',\n",
       " 'rapist',\n",
       " 'commune',\n",
       " 'maid',\n",
       " 'lady',\n",
       " 'home',\n",
       " 'ep1',\n",
       " 'lot',\n",
       " 'gang',\n",
       " 'member',\n",
       " 'wrist',\n",
       " 'oil',\n",
       " 'well',\n",
       " 'wrist',\n",
       " 'insecurity',\n",
       " 'issue',\n",
       " 'blowjob',\n",
       " 'episode',\n",
       " 'people',\n",
       " 'car',\n",
       " 'motorcyclosexual',\n",
       " 'tube',\n",
       " 'feeling',\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing seasons datasets (getting nouns and adjs)\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "\n",
    "s1_nouns = [[word.lemma_.lower() for word in text if word.pos_ == 'NOUN'] for text in s1.processed_texts]\n",
    "s1_nouns = flatten(s1_nouns)\n",
    "\n",
    "s1_adjs = [[word.lemma_.lower() for word in text if word.pos_ == 'ADJ'] for text in s1.processed_texts]\n",
    "s1_adjs = flatten(s1_adjs)\n",
    "\n",
    "s2_nouns = [[word.lemma_.lower() for word in text if word.pos_ == 'NOUN'] for text in s2.processed_texts]\n",
    "s2_nouns = flatten(s2_nouns)\n",
    "\n",
    "s2_adjs = [[word.lemma_.lower() for word in text if word.pos_ == 'ADJ'] for text in s2.processed_texts]\n",
    "s2_adjs = flatten(s2_adjs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Think of a word that will definitely occur more often in the first subset than in the second (or vice versa), and think of a word that will not differ that much. Compare the frequency of those words using the log-likelihood measure (so you’ll calculate the LLR twice – once for each word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counters for each season\n",
    "counts_c1 = Counter(s1_adjs)\n",
    "counts_c2 = Counter(s2_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood:  -10.184661652600141\n",
      "p-value 0.0014161370413705767\n"
     ]
    }
   ],
   "source": [
    "# since many people was disappointed with the second season, I will how distinctive the word 'bad' is\n",
    "differ_word = 'bad'\n",
    "freq_c1 = counts_c1[differ_word]\n",
    "freq_c2 = counts_c2[differ_word]\n",
    "\n",
    "freq_c1_other = sum(counts_c1.values()) - freq_c1\n",
    "freq_c2_other = sum(counts_c2.values()) - freq_c2\n",
    "\n",
    "llr, p_value,_,_ = chi2_contingency([[freq_c1, freq_c2], \n",
    "                  [freq_c1_other, freq_c2_other]],\n",
    "                  lambda_='log-likelihood')\n",
    "if freq_c2 / freq_c2_other > freq_c1 / freq_c1_other: # adjust sign of llr\n",
    "    llr = -llr\n",
    "    \n",
    "print(\"Log-likelihood: \", llr)\n",
    "print('p-value', p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-likelihood:  0.11104046481874308\n",
      "p-value 0.7389626766715127\n"
     ]
    }
   ],
   "source": [
    "# a word that has normally been used to describe this show is 'dark'. let's see how similar the use of the word is\n",
    "differ_word = 'dark'\n",
    "freq_c1 = counts_c1[differ_word]\n",
    "freq_c2 = counts_c2[differ_word]\n",
    "\n",
    "freq_c1_other = sum(counts_c1.values()) - freq_c1\n",
    "freq_c2_other = sum(counts_c2.values()) - freq_c2\n",
    "\n",
    "llr, p_value,_,_ = chi2_contingency([[freq_c1, freq_c2], \n",
    "                  [freq_c1_other, freq_c2_other]],\n",
    "                  lambda_='log-likelihood') \n",
    "\n",
    "if freq_c2 / freq_c2_other > freq_c1 / freq_c1_other: # adjust sign of llr\n",
    "    llr = -llr\n",
    "    \n",
    "print(\"Log-likelihood: \", llr)\n",
    "print('p-value', p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Get the most distinctive words of the first subset compared to second subset, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinctive_words(target_corpus, reference_corpus):\n",
    "    counts_c1 = Counter(target_corpus) # don't forget to flatten your texts!\n",
    "    counts_c2 = Counter(reference_corpus)\n",
    "    vocabulary = set(list(counts_c1.keys()) + list(counts_c2.keys()))\n",
    "    freq_c1_total = sum(counts_c1.values()) \n",
    "    freq_c2_total = sum(counts_c2.values()) \n",
    "    results = []\n",
    "    for word in vocabulary:\n",
    "        freq_c1 = counts_c1[word]\n",
    "        freq_c2 = counts_c2[word]\n",
    "        freq_c1_other = freq_c1_total - freq_c1\n",
    "        freq_c2_other = freq_c2_total - freq_c2\n",
    "        llr, p_value,_,_ = chi2_contingency([[freq_c1, freq_c2], \n",
    "                      [freq_c1_other, freq_c2_other]],\n",
    "                      lambda_='log-likelihood') \n",
    "        if freq_c2 / freq_c2_other > freq_c1 / freq_c1_other:\n",
    "            llr = -llr\n",
    "        result = {'word':word, \n",
    "                    'llr':llr,\n",
    "                    'p_value': p_value}\n",
    "        results.append(result)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_comp = distinctive_words(s1_adjs, s2_adjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>llr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>yellow</td>\n",
       "      <td>28.715118</td>\n",
       "      <td>8.384663e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>strange</td>\n",
       "      <td>6.363820</td>\n",
       "      <td>1.164705e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>sorry</td>\n",
       "      <td>5.735741</td>\n",
       "      <td>1.662306e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>little</td>\n",
       "      <td>5.052105</td>\n",
       "      <td>2.459604e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>many</td>\n",
       "      <td>4.644083</td>\n",
       "      <td>3.116078e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>well</td>\n",
       "      <td>-6.443919</td>\n",
       "      <td>1.113327e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>weird</td>\n",
       "      <td>-8.296338</td>\n",
       "      <td>3.972511e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>618</th>\n",
       "      <td>mexican</td>\n",
       "      <td>-8.629000</td>\n",
       "      <td>3.308532e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>bad</td>\n",
       "      <td>-10.184662</td>\n",
       "      <td>1.416137e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>hard</td>\n",
       "      <td>-11.534528</td>\n",
       "      <td>6.831543e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>926 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        word        llr       p_value\n",
       "24    yellow  28.715118  8.384663e-08\n",
       "645  strange   6.363820  1.164705e-02\n",
       "766    sorry   5.735741  1.662306e-02\n",
       "287   little   5.052105  2.459604e-02\n",
       "363     many   4.644083  3.116078e-02\n",
       "..       ...        ...           ...\n",
       "1       well  -6.443919  1.113327e-02\n",
       "400    weird  -8.296338  3.972511e-03\n",
       "618  mexican  -8.629000  3.308532e-03\n",
       "649      bad -10.184662  1.416137e-03\n",
       "101     hard -11.534528  6.831543e-04\n",
       "\n",
       "[926 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_comp.sort_values('llr', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most distinctive adjective in the case of the first season seems to be 'yellow', which I assume is connected to character of 'the yellow king' in the first season.\n",
    "\n",
    "In the case of the second season, it seems to be 'hard', which I cannot connect to any particular detail of the show. However, the incidence of the adjective 'bad' is also high, which is explained by the difference in the reception of both seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Get the most distinctive words of the first subset compared to all the posts that are not in the first subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_not_1 = df[(df.title != 'True Detective') & (df.year != 2014)].processed_texts\n",
    "subset_not_1 = [[word.lemma_.lower() for word in text if word.pos_ == 'NOUN'] for text in subset_not_1]\n",
    "subset_not_1 = flatten(subset_not_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_tds1 = distinctive_words(s1_nouns, subset_not_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>llr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11431</th>\n",
       "      <td>cult</td>\n",
       "      <td>218.527800</td>\n",
       "      <td>1.894579e-49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>detective</td>\n",
       "      <td>130.533913</td>\n",
       "      <td>3.131365e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2628</th>\n",
       "      <td>rust</td>\n",
       "      <td>96.760337</td>\n",
       "      <td>7.824800e-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8315</th>\n",
       "      <td>marty</td>\n",
       "      <td>87.178438</td>\n",
       "      <td>9.915894e-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10033</th>\n",
       "      <td>case</td>\n",
       "      <td>59.897203</td>\n",
       "      <td>9.994338e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2863</th>\n",
       "      <td>daughter</td>\n",
       "      <td>54.682291</td>\n",
       "      <td>1.416772e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>circle</td>\n",
       "      <td>53.055547</td>\n",
       "      <td>3.242480e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5188</th>\n",
       "      <td>reggie</td>\n",
       "      <td>43.580594</td>\n",
       "      <td>4.068535e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11427</th>\n",
       "      <td>lawn</td>\n",
       "      <td>43.427424</td>\n",
       "      <td>4.399786e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2326</th>\n",
       "      <td>finale</td>\n",
       "      <td>35.646221</td>\n",
       "      <td>2.366064e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>drawing</td>\n",
       "      <td>35.609153</td>\n",
       "      <td>2.411518e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12670</th>\n",
       "      <td>darkness</td>\n",
       "      <td>29.247921</td>\n",
       "      <td>6.368452e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9992</th>\n",
       "      <td>nose</td>\n",
       "      <td>26.758265</td>\n",
       "      <td>2.305634e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>mask</td>\n",
       "      <td>26.436259</td>\n",
       "      <td>2.723792e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3939</th>\n",
       "      <td>scarring</td>\n",
       "      <td>24.594952</td>\n",
       "      <td>7.073678e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11724</th>\n",
       "      <td>family</td>\n",
       "      <td>20.359211</td>\n",
       "      <td>6.418340e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1782</th>\n",
       "      <td>tape</td>\n",
       "      <td>20.304728</td>\n",
       "      <td>6.603721e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4266</th>\n",
       "      <td>priest</td>\n",
       "      <td>19.941003</td>\n",
       "      <td>7.986889e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>star</td>\n",
       "      <td>19.299006</td>\n",
       "      <td>1.117645e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>hallucination</td>\n",
       "      <td>19.035785</td>\n",
       "      <td>1.282899e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word         llr       p_value\n",
       "11431           cult  218.527800  1.894579e-49\n",
       "10466      detective  130.533913  3.131365e-30\n",
       "2628            rust   96.760337  7.824800e-23\n",
       "8315           marty   87.178438  9.915894e-21\n",
       "10033           case   59.897203  9.994338e-15\n",
       "2863        daughter   54.682291  1.416772e-13\n",
       "3419          circle   53.055547  3.242480e-13\n",
       "5188          reggie   43.580594  4.068535e-11\n",
       "11427           lawn   43.427424  4.399786e-11\n",
       "2326          finale   35.646221  2.366064e-09\n",
       "4933         drawing   35.609153  2.411518e-09\n",
       "12670       darkness   29.247921  6.368452e-08\n",
       "9992            nose   26.758265  2.305634e-07\n",
       "3709            mask   26.436259  2.723792e-07\n",
       "3939        scarring   24.594952  7.073678e-07\n",
       "11724         family   20.359211  6.418340e-06\n",
       "1782            tape   20.304728  6.603721e-06\n",
       "4266          priest   19.941003  7.986889e-06\n",
       "7577            star   19.299006  1.117645e-05\n",
       "74     hallucination   19.035785  1.282899e-05"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_tds1.sort_values('llr', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Get the most distinctive words of the second subset compared to all the posts that are not in the second subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_not_2 = df[(df.title != 'True Detective') & (df.year != 2015)].processed_texts\n",
    "subset_not_2 = [[word.lemma_.lower() for word in text if word.pos_ == 'NOUN'] for text in subset_not_2]\n",
    "subset_not_2 = flatten(subset_not_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_tds2 = distinctive_words(s2_nouns, subset_not_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>llr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9162</th>\n",
       "      <td>diamond</td>\n",
       "      <td>129.478104</td>\n",
       "      <td>5.330120e-30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9978</th>\n",
       "      <td>detective</td>\n",
       "      <td>68.456017</td>\n",
       "      <td>1.297393e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3252</th>\n",
       "      <td>noir</td>\n",
       "      <td>60.266719</td>\n",
       "      <td>8.283621e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1116</th>\n",
       "      <td>mayor</td>\n",
       "      <td>60.159572</td>\n",
       "      <td>8.747047e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10925</th>\n",
       "      <td>cult</td>\n",
       "      <td>57.543276</td>\n",
       "      <td>3.306243e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3538</th>\n",
       "      <td>mask</td>\n",
       "      <td>57.013196</td>\n",
       "      <td>4.328975e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>officer</td>\n",
       "      <td>55.137280</td>\n",
       "      <td>1.123987e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8280</th>\n",
       "      <td>drive</td>\n",
       "      <td>51.171583</td>\n",
       "      <td>8.463501e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11322</th>\n",
       "      <td>deal</td>\n",
       "      <td>47.046481</td>\n",
       "      <td>6.932303e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10040</th>\n",
       "      <td>corruption</td>\n",
       "      <td>45.422511</td>\n",
       "      <td>1.587987e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11768</th>\n",
       "      <td>season</td>\n",
       "      <td>45.013781</td>\n",
       "      <td>1.956527e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4104</th>\n",
       "      <td>rapist</td>\n",
       "      <td>44.160522</td>\n",
       "      <td>3.025213e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9856</th>\n",
       "      <td>gangster</td>\n",
       "      <td>41.390898</td>\n",
       "      <td>1.246363e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10403</th>\n",
       "      <td>commune</td>\n",
       "      <td>40.356825</td>\n",
       "      <td>2.115650e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6622</th>\n",
       "      <td>cop</td>\n",
       "      <td>40.243772</td>\n",
       "      <td>2.241688e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8113</th>\n",
       "      <td>orgy</td>\n",
       "      <td>39.063700</td>\n",
       "      <td>4.102005e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>party</td>\n",
       "      <td>38.779248</td>\n",
       "      <td>4.745461e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4881</th>\n",
       "      <td>shootout</td>\n",
       "      <td>38.186284</td>\n",
       "      <td>6.430284e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11420</th>\n",
       "      <td>guy</td>\n",
       "      <td>36.778044</td>\n",
       "      <td>1.323727e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11443</th>\n",
       "      <td>bird</td>\n",
       "      <td>31.106201</td>\n",
       "      <td>2.442898e-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             word         llr       p_value\n",
       "9162      diamond  129.478104  5.330120e-30\n",
       "9978    detective   68.456017  1.297393e-16\n",
       "3252         noir   60.266719  8.283621e-15\n",
       "1116        mayor   60.159572  8.747047e-15\n",
       "10925        cult   57.543276  3.306243e-14\n",
       "3538         mask   57.013196  4.328975e-14\n",
       "5226      officer   55.137280  1.123987e-13\n",
       "8280        drive   51.171583  8.463501e-13\n",
       "11322        deal   47.046481  6.932303e-12\n",
       "10040  corruption   45.422511  1.587987e-11\n",
       "11768      season   45.013781  1.956527e-11\n",
       "4104       rapist   44.160522  3.025213e-11\n",
       "9856     gangster   41.390898  1.246363e-10\n",
       "10403     commune   40.356825  2.115650e-10\n",
       "6622          cop   40.243772  2.241688e-10\n",
       "8113         orgy   39.063700  4.102005e-10\n",
       "2069        party   38.779248  4.745461e-10\n",
       "4881     shootout   38.186284  6.430284e-10\n",
       "11420         guy   36.778044  1.323727e-09\n",
       "11443        bird   31.106201  2.442898e-08"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_tds2.sort_values('llr', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get the most distinctive words of the first and the second subset, compared to all the posts that are neither in the first nor in the second subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_1_2 = df[((df.title == 'True Detective') & (df.year == 2014))|((df.title == 'True Detective') & (df.year == 2015))].processed_texts\n",
    "subset_1_2 = [[word.lemma_.lower() for word in text if word.pos_ == 'NOUN'] for text in subset_1_2]\n",
    "subset_1_2 = flatten(subset_1_2)\n",
    "\n",
    "subset_not_1_2 = df[(df.title != 'True Detective') & ~(df.year.isin([2014, 2015]))].processed_texts\n",
    "subset_not_1_2 = [[word.lemma_.lower() for word in text if word.pos_ == 'NOUN'] for text in subset_not_1_2]\n",
    "subset_not_1_2 = flatten(subset_not_1_2)\n",
    "\n",
    "results_df_both = distinctive_words(subset_1_2, subset_not_1_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>llr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10952</th>\n",
       "      <td>cult</td>\n",
       "      <td>235.450754</td>\n",
       "      <td>3.860851e-53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>detective</td>\n",
       "      <td>172.058586</td>\n",
       "      <td>2.627642e-39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9191</th>\n",
       "      <td>diamond</td>\n",
       "      <td>109.190289</td>\n",
       "      <td>1.474307e-25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3546</th>\n",
       "      <td>mask</td>\n",
       "      <td>80.368808</td>\n",
       "      <td>3.106607e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2521</th>\n",
       "      <td>rust</td>\n",
       "      <td>80.091231</td>\n",
       "      <td>3.575156e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3270</th>\n",
       "      <td>circle</td>\n",
       "      <td>67.151803</td>\n",
       "      <td>2.513853e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>noir</td>\n",
       "      <td>56.997630</td>\n",
       "      <td>4.363374e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7965</th>\n",
       "      <td>marty</td>\n",
       "      <td>56.913400</td>\n",
       "      <td>4.554321e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5243</th>\n",
       "      <td>officer</td>\n",
       "      <td>46.994896</td>\n",
       "      <td>7.117182e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1119</th>\n",
       "      <td>mayor</td>\n",
       "      <td>46.994896</td>\n",
       "      <td>7.117182e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6648</th>\n",
       "      <td>cop</td>\n",
       "      <td>46.892920</td>\n",
       "      <td>7.497308e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>drive</td>\n",
       "      <td>42.147588</td>\n",
       "      <td>8.463826e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11453</th>\n",
       "      <td>guy</td>\n",
       "      <td>36.907901</td>\n",
       "      <td>1.238431e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>case</td>\n",
       "      <td>36.608320</td>\n",
       "      <td>1.444140e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>finale</td>\n",
       "      <td>36.050063</td>\n",
       "      <td>1.923126e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10072</th>\n",
       "      <td>corruption</td>\n",
       "      <td>35.837124</td>\n",
       "      <td>2.145211e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7635</th>\n",
       "      <td>investigation</td>\n",
       "      <td>35.639943</td>\n",
       "      <td>2.373702e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10435</th>\n",
       "      <td>commune</td>\n",
       "      <td>34.218837</td>\n",
       "      <td>4.924992e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4114</th>\n",
       "      <td>rapist</td>\n",
       "      <td>33.842697</td>\n",
       "      <td>5.975297e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10948</th>\n",
       "      <td>lawn</td>\n",
       "      <td>33.122366</td>\n",
       "      <td>8.653755e-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word         llr       p_value\n",
       "10952           cult  235.450754  3.860851e-53\n",
       "10010      detective  172.058586  2.627642e-39\n",
       "9191         diamond  109.190289  1.474307e-25\n",
       "3546            mask   80.368808  3.106607e-19\n",
       "2521            rust   80.091231  3.575156e-19\n",
       "3270          circle   67.151803  2.513853e-16\n",
       "3259            noir   56.997630  4.363374e-14\n",
       "7965           marty   56.913400  4.554321e-14\n",
       "5243         officer   46.994896  7.117182e-12\n",
       "1119           mayor   46.994896  7.117182e-12\n",
       "6648             cop   46.892920  7.497308e-12\n",
       "8308           drive   42.147588  8.463826e-11\n",
       "11453            guy   36.907901  1.238431e-09\n",
       "9595            case   36.608320  1.444140e-09\n",
       "2234          finale   36.050063  1.923126e-09\n",
       "10072     corruption   35.837124  2.145211e-09\n",
       "7635   investigation   35.639943  2.373702e-09\n",
       "10435        commune   34.218837  4.924992e-09\n",
       "4114          rapist   33.842697  5.975297e-09\n",
       "10948           lawn   33.122366  8.653755e-09"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_both.sort_values('llr', ascending=False).head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
