{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. First, tokenize the sentence yourself (i.e. manually convert this string toa list of tokens) without looking at any of the methods in the manual.Then, use at least two different (computational) methods for tokenization. Compare the results: describe the differences and try to explain them given what you know about the way they work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'We’re where we were when W.W.W. Wonka (1940-2012) was when he was selected prime minister of the U.K. with 50.23% of the votes. For more information, see: www.we-want-wonka.co.uk/2012'\n",
    "punctuation = re.compile(r'[!\"#$%&\\'\\(\\)\\*\\+\\,\\-\\.\\/:\\;\\<=\\>\\?@\\[\\\\\\]\\^\\_`\\{|\\}~]')\n",
    "\n",
    "# No methods used\n",
    "def tokenize(text):\n",
    "    punctuation.sub(text, '')\n",
    "    text = text.lower() \n",
    "    text = text.split()\n",
    "    return text\n",
    "\n",
    "result1 = tokenize(sample)\n",
    "\n",
    "# NLTK method\n",
    "tokenizer = RegexpTokenizer('\\w+')\n",
    "result2 = tokenizer.tokenize(sample.lower())\n",
    "\n",
    "# Spacy method\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "processed_text = nlp(sample.lower())\n",
    "\n",
    "result3 = [token.text for token in processed_text if not token.is_punct]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put together, the different tokens lists look like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we’re</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>where</td>\n",
       "      <td>re</td>\n",
       "      <td>’re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>where</td>\n",
       "      <td>where</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>were</td>\n",
       "      <td>we</td>\n",
       "      <td>we</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>when</td>\n",
       "      <td>were</td>\n",
       "      <td>were</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>w.w.w.</td>\n",
       "      <td>when</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>wonka</td>\n",
       "      <td>w</td>\n",
       "      <td>w.w.w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(1940-2012)</td>\n",
       "      <td>w</td>\n",
       "      <td>wonka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>was</td>\n",
       "      <td>w</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>when</td>\n",
       "      <td>wonka</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>he</td>\n",
       "      <td>1940</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>was</td>\n",
       "      <td>2012</td>\n",
       "      <td>when</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>selected</td>\n",
       "      <td>was</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>prime</td>\n",
       "      <td>when</td>\n",
       "      <td>was</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minister</td>\n",
       "      <td>he</td>\n",
       "      <td>selected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>of</td>\n",
       "      <td>was</td>\n",
       "      <td>prime</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>the</td>\n",
       "      <td>selected</td>\n",
       "      <td>minister</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>u.k.</td>\n",
       "      <td>prime</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>with</td>\n",
       "      <td>minister</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>50.23%</td>\n",
       "      <td>of</td>\n",
       "      <td>u.k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>with</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>the</td>\n",
       "      <td>u</td>\n",
       "      <td>50.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>votes.</td>\n",
       "      <td>k</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>for</td>\n",
       "      <td>with</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>more</td>\n",
       "      <td>50</td>\n",
       "      <td>votes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>information,</td>\n",
       "      <td>23</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>see:</td>\n",
       "      <td>of</td>\n",
       "      <td>more</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>www.we-want-wonka.co.uk/2012</td>\n",
       "      <td>the</td>\n",
       "      <td>information</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>None</td>\n",
       "      <td>votes</td>\n",
       "      <td>see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>None</td>\n",
       "      <td>for</td>\n",
       "      <td>www.we-want-wonka.co.uk/2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>None</td>\n",
       "      <td>more</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>None</td>\n",
       "      <td>information</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>None</td>\n",
       "      <td>see</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>None</td>\n",
       "      <td>www</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>None</td>\n",
       "      <td>we</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>None</td>\n",
       "      <td>want</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>None</td>\n",
       "      <td>wonka</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>None</td>\n",
       "      <td>co</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>None</td>\n",
       "      <td>uk</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>None</td>\n",
       "      <td>2012</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               0            1                             2\n",
       "0                          we’re           we                            we\n",
       "1                          where           re                           ’re\n",
       "2                             we        where                         where\n",
       "3                           were           we                            we\n",
       "4                           when         were                          were\n",
       "5                         w.w.w.         when                          when\n",
       "6                          wonka            w                         w.w.w\n",
       "7                    (1940-2012)            w                         wonka\n",
       "8                            was            w                          1940\n",
       "9                           when        wonka                          2012\n",
       "10                            he         1940                           was\n",
       "11                           was         2012                          when\n",
       "12                      selected          was                            he\n",
       "13                         prime         when                           was\n",
       "14                      minister           he                      selected\n",
       "15                            of          was                         prime\n",
       "16                           the     selected                      minister\n",
       "17                          u.k.        prime                            of\n",
       "18                          with     minister                           the\n",
       "19                        50.23%           of                           u.k\n",
       "20                            of          the                          with\n",
       "21                           the            u                         50.23\n",
       "22                        votes.            k                            of\n",
       "23                           for         with                           the\n",
       "24                          more           50                         votes\n",
       "25                  information,           23                           for\n",
       "26                          see:           of                          more\n",
       "27  www.we-want-wonka.co.uk/2012          the                   information\n",
       "28                          None        votes                           see\n",
       "29                          None          for  www.we-want-wonka.co.uk/2012\n",
       "30                          None         more                          None\n",
       "31                          None  information                          None\n",
       "32                          None          see                          None\n",
       "33                          None          www                          None\n",
       "34                          None           we                          None\n",
       "35                          None         want                          None\n",
       "36                          None        wonka                          None\n",
       "37                          None           co                          None\n",
       "38                          None           uk                          None\n",
       "39                          None         2012                          None"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data=[result1, result2, result3]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differences:\n",
    "\n",
    "- result1 splits on any of the characters included in the punctuation list.\n",
    "\n",
    "- result2 splits the sample text in sequences that match the indicated pattern '\\w+' (sequence of word characters). It is equivalent to the method 'findall' from the re library.\n",
    "\n",
    "- result3 is the library spaCy particular method of tokenization. It adds some special cases to the standard tokenization. For example, it seems to retrieve the full value of numbers that have a decimal part (50.23), or abbreviations separated by periods (U.K). Also, it retrieves full links to websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. In 2017, a medium article compared the word counts of four different tools (Word, Word Online, Google Docs, LibreOffice). All tools produced different word counts. The author concluded: ’I’m not counting manually to find out who is correct, so I am declaring them all rubbish’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens obtained with custom tokenize function: 28\n",
      "Total number of tokens obtained with RegexpTokenizer: 40\n",
      "Total number of tokens obtained with spaCy tokenizer: 30\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of tokens obtained with custom tokenize function: {len(result1)}\")\n",
    "print(f\"Total number of tokens obtained with RegexpTokenizer: {len(result2)}\")\n",
    "print(f\"Total number of tokens obtained with spaCy tokenizer: {len(result3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the tokens obtained with spaCy seem the most reliable to me, I would say that the most appropriate length is 30 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Compare the occurrences of the words ’we’, ’www’ between tokenization methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom tokenize function retrieves: {\"we’re\"}, {\"w.w.w\"}\n",
    "\n",
    "RegexpTokenizer retrieves: {\"we\", \"re\"}, {\"w\", \"w\", \"w\"}\n",
    "\n",
    "spaCy tokenizer retrieves: {\"we\", \"’re\"}, {\"w.w.w\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Compare the ten most frequent words based on the different (including your own) tokenization methods. Do you think these differences will matter when you have millions of texts, and not some nonsense text solely designed to make you reflect on the impact of preprocessing methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('when', 2), ('was', 2), ('of', 2), ('the', 2), ('we’re', 1)],\n",
       " [('we', 3), ('w', 3), ('when', 2), ('wonka', 2), ('2012', 2)],\n",
       " [('we', 2), ('when', 2), ('was', 2), ('of', 2), ('the', 2)])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_1 = Counter(result1)\n",
    "counter_2 = Counter(result2)\n",
    "counter_3 = Counter(result3)\n",
    "\n",
    "counter_1.most_common(5), counter_2.most_common(5), counter_3.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the difference is so substantial in such a small text, it is very likely that they will matter in bigger samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickle file\n",
    "df = pd.read_pickle(r'discussions.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many posts?\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many lemmatized nouns and lemmatized adjectives?\n",
    "\n",
    "text = df[\"post\"].to_string()\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = len(text)\n",
    "processed_text = nlp(text.lower())\n",
    "\n",
    "# Lemmatized nouns\n",
    "lem_nouns = [token.lemma_ for token in processed_text if token.pos_ == \"NOUN\"]\n",
    "Counter(lem_nouns).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
