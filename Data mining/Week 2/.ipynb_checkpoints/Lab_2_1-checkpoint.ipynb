{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WazUkLMT9rnz"
   },
   "source": [
    "# Data Mining: lab 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CAOyw-K19rn0",
    "outputId": "c0bd38c9-a26e-44fb-990b-788ef39cfe4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.5.1)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jfC6L2Ns9rn1"
   },
   "outputs": [],
   "source": [
    "# Load data as a dataframe\n",
    "\n",
    "df = pd.read_pickle('labeled_tweets.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "3eYkQfte9rn1",
    "outputId": "94d25ca4-4cd6-4684-d01c-28d50d7ac4fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2882\n",
      "Number of attributes: 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @WaysMeansCmte: Republican Senators need to...</td>\n",
       "      <td>Laid-off workers set up soup kitchens in front...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jeff Van Drew sold out his district and his co...</td>\n",
       "      <td>Pitch in to help Amy Kennedy defeat Jeff Van D...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Speaker Pelosi has failed the American people—...</td>\n",
       "      <td>House Minority Leader McCarthy: Pelosi touts D...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>To learn more about global efforts to #EndPoli...</td>\n",
       "      <td>Home | End Polio. With your help, we can end p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @realDailyWire: BREAKING: Hunter Biden Rece...</td>\n",
       "      <td>Hunter Biden Received Millions From Wife Of Ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              text_a  ... label\n",
       "0  RT @WaysMeansCmte: Republican Senators need to...  ...     2\n",
       "1  Jeff Van Drew sold out his district and his co...  ...     0\n",
       "2  Speaker Pelosi has failed the American people—...  ...     1\n",
       "3  To learn more about global efforts to #EndPoli...  ...     1\n",
       "4  RT @realDailyWire: BREAKING: Hunter Biden Rece...  ...     0\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic info\n",
    "\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of attributes: {df.shape[1]}\")\n",
    "\n",
    "# Overview of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "yrBeIT-89rn5",
    "outputId": "55608bbc-b6ac-4a28-941d-e8a403b0574a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_a</th>\n",
       "      <th>text_b</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1275</td>\n",
       "      <td>1275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1377</td>\n",
       "      <td>1377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>230</td>\n",
       "      <td>230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_a  text_b\n",
       "label                \n",
       "0        1275    1275\n",
       "1        1377    1377\n",
       "2         230     230"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the dataset: how many tweets are there per different value of 'label'?\n",
    "\n",
    "df.groupby(by=\"label\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofx9TWJU9rn6"
   },
   "source": [
    "### Logistic Regession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "WYfw6oRV9rn6"
   },
   "outputs": [],
   "source": [
    "# Split the test in training and test set. For this BOW analysis we will use the words contained in the text_a features.\n",
    "X = list(df.text_a.values)\n",
    "y = list(df.label.values)\n",
    "labels = ['affitmative', 'negotiated', 'oppositional']\n",
    "\n",
    "X_train_str, X_test_str, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H0ZPfb2_9rn6"
   },
   "outputs": [],
   "source": [
    "# Vectorize the string features\n",
    "cv = CountVectorizer()\n",
    "cv.fit(X_train_str) # create the vocabulary\n",
    "\n",
    "X_train = cv.transform(X_train_str)\n",
    "X_test = cv.transform(X_test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NhQec0C9rn6",
    "outputId": "2853293f-77c4-49a3-91a2-afa31db193af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train the model\n",
    "lr = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xT-B_5xr9rn6",
    "outputId": "fc80d183-354e-470b-e519-c2cc91e01e11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " affitmative       0.57      0.62      0.60       256\n",
      "  negotiated       0.55      0.59      0.57       270\n",
      "oppositional       0.20      0.04      0.07        51\n",
      "\n",
      "    accuracy                           0.55       577\n",
      "   macro avg       0.44      0.42      0.41       577\n",
      "weighted avg       0.53      0.55      0.54       577\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[159  95   2]\n",
      " [105 159   6]\n",
      " [ 14  35   2]]\n"
     ]
    }
   ],
   "source": [
    "# Determine how well the algorithm predicts the target variable\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "print(\"Confusion matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYvTcUjE9rn7",
    "outputId": "e1f02c3a-69ef-40ed-861c-b53bbc393610"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " affitmative       0.43      0.32      0.37       256\n",
      "  negotiated       0.51      0.34      0.41       270\n",
      "oppositional       0.11      0.43      0.17        51\n",
      "\n",
      "    accuracy                           0.34       577\n",
      "   macro avg       0.35      0.37      0.32       577\n",
      "weighted avg       0.44      0.34      0.37       577\n",
      "\n",
      "[[83 78 95]\n",
      " [91 92 87]\n",
      " [20  9 22]]\n"
     ]
    }
   ],
   "source": [
    "# How does it perform with a random set of label values?\n",
    "y_rand = np.random.randint(0, 3, len(y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_rand, target_names=labels))\n",
    "print(confusion_matrix(y_test, y_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-riTG_bXRNN0"
   },
   "source": [
    "Our classifier gives a more accurate prediction that a random guess of the label. Another fact that justifes it is that the f1-score is higher than 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "id": "Kgn5xMVt9rn7",
    "outputId": "f0336004-1775-47c1-f2d8-030418a530e8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5583</th>\n",
       "      <td>0.738226</td>\n",
       "      <td>helping</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10305</th>\n",
       "      <td>0.713530</td>\n",
       "      <td>rt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>0.704435</td>\n",
       "      <td>complete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3189</th>\n",
       "      <td>0.635509</td>\n",
       "      <td>cruz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1820</th>\n",
       "      <td>0.607602</td>\n",
       "      <td>below</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6493</th>\n",
       "      <td>0.595849</td>\n",
       "      <td>job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>0.590124</td>\n",
       "      <td>generation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11553</th>\n",
       "      <td>0.585061</td>\n",
       "      <td>team</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7098</th>\n",
       "      <td>0.581903</td>\n",
       "      <td>live</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>0.578070</td>\n",
       "      <td>confirmation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           coef          word\n",
       "5583   0.738226       helping\n",
       "10305  0.713530            rt\n",
       "2839   0.704435      complete\n",
       "3189   0.635509          cruz\n",
       "1820   0.607602         below\n",
       "6493   0.595849           job\n",
       "5114   0.590124    generation\n",
       "11553  0.585061          team\n",
       "7098   0.581903          live\n",
       "2887   0.578070  confirmation"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try to interpret the model. Based on the words with the highest coefficients, what do you think the model has learned?\n",
    "\n",
    "vocabulary = cv.get_feature_names()\n",
    "regression_coefficients = lr.coef_[0] # get the LR weights\n",
    "vocab_coef_combined = list(zip(regression_coefficients, vocabulary)) # this combines two separate lists [1, 2], ['word1', 'word2'] into one list [[1, 'word1'], [2, 'word2']]\n",
    "\n",
    "feature_importance = pd.DataFrame(vocab_coef_combined,\n",
    "                      columns=['coef', 'word'])\n",
    "feature_importance.sort_values('coef', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l3YQ6ale9rn7"
   },
   "outputs": [],
   "source": [
    "# Use TF-IDF features instead of raw counts. Does it improve the performance of your model?\n",
    "\n",
    "tfidf = TfidfVectorizer() \n",
    "\n",
    "tfidf.fit(X_train_str) # create the vocabulary\n",
    "\n",
    "X_train_idf = tfidf.transform(X_train_str)\n",
    "X_test = tfidf.transform(X_test_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OiwJ_xCp9rn7",
    "outputId": "f33ea689-59ce-41d5-dd90-323334270117"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " affitmative       0.47      0.96      0.63       256\n",
      "  negotiated       0.63      0.11      0.19       270\n",
      "oppositional       0.00      0.00      0.00        51\n",
      "\n",
      "    accuracy                           0.48       577\n",
      "   macro avg       0.37      0.36      0.27       577\n",
      "weighted avg       0.50      0.48      0.37       577\n",
      "\n",
      "Confusion matrix:\n",
      "\n",
      "[[246  10   0]\n",
      " [239  31   0]\n",
      " [ 43   8   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\\n\")\n",
    "print(classification_report(y_test, y_pred, target_names=labels))\n",
    "print(\"Confusion matrix:\\n\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQuCprLA9rn8"
   },
   "source": [
    "Using TF-IDF vectorizer doesn't improve the performance of the model: both macro and weighted averages of precision, recall and f1 score are lower than when using raw frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNMymS4g9rn8"
   },
   "source": [
    "### BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKTe7kbu9rn8"
   },
   "source": [
    "#### Fine-tune a DistilBERT model on our task and evaluate its performance: does it perform any better than using a logistic regression classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JytZLfQ49rn8"
   },
   "outputs": [],
   "source": [
    "# Create sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "b0lxy8Ll9rn8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128) # convert input strings to BERT encodings\n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True,  max_length=128)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).shuffle(1000).batch(16) # convert the encodings to Tensorflow objects\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val\n",
    ")).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tXRQlgHf9rn8",
    "outputId": "2d33f5b9-ea53-4a8c-e4e2-4877da7bcd12"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-cased', \n",
    "                                                           num_labels=len(labels))\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, \n",
    "                      mode='min', baseline=None, \n",
    "                      restore_best_weights=True)]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PATScwaq9rn8",
    "outputId": "cdc697c6-8c91-41ae-967f-3e5a72ca5a29"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "145/145 [==============================] - 34s 238ms/step - loss: 0.9161 - val_loss: 0.9050\n",
      "Epoch 2/10\n",
      "145/145 [==============================] - 35s 239ms/step - loss: 0.8809 - val_loss: 0.9111\n",
      "Epoch 3/10\n",
      "145/145 [==============================] - 34s 232ms/step - loss: 0.8269 - val_loss: 0.8565\n",
      "Epoch 4/10\n",
      "145/145 [==============================] - 34s 234ms/step - loss: 0.7147 - val_loss: 0.9317\n",
      "Epoch 5/10\n",
      "145/145 [==============================] - 34s 234ms/step - loss: 0.5591 - val_loss: 1.0203\n",
      "Epoch 6/10\n",
      "145/145 [==============================] - 34s 234ms/step - loss: 0.3473 - val_loss: 1.2978\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1cb161be0>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model for ten epochs\n",
    "model.fit(train_dataset, epochs=10, callbacks=callbacks, validation_data=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mog8o1YM9rn8",
    "outputId": "bba8eeca-86d7-473c-e4f5-73f73f268968"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.48      0.55       116\n",
      "           1       0.59      0.80      0.68       147\n",
      "           2       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.60       288\n",
      "   macro avg       0.41      0.43      0.41       288\n",
      "weighted avg       0.56      0.60      0.57       288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "logits = model.predict(test_dataset)\n",
    "y_preds = np.argmax(logits[0], axis=1)\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOlXNCBdV10G"
   },
   "source": [
    "The results aren't overwhelmingly better than the other's for any of the algorithms. However, we will favor BERT against logistic regression because the f1-score of the accuracy is higher (0.62 > 0.55)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OS_ZlhSZZ43U"
   },
   "source": [
    "#### Instead of using a DistilBERT which is trained of all kinds of texts, we could also make use of BERTweet, which is – the name says it all – specifically trained on tweets. To use BERTweet, we’ll have to make a few adjustments to the code in the manual [also make sure you install the emoji package (!pip install emoji)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MsO2pkt7Z2JM",
    "outputId": "007b6e2d-75fe-44fc-8fa8-39fe71302304"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C7qOOBlaZ_IS",
    "outputId": "354e01a3-547c-448f-ff57-c37b65173587"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Some layers from the model checkpoint at vinai/bertweet-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at vinai/bertweet-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5, random_state=1)\n",
    "\n",
    "# The two steps below are different for BERTweet model\n",
    "# Load bertweet tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', normalization=True)\n",
    "\n",
    "# Load bertweet model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('vinai/bertweet-base', num_labels=len(labels))\n",
    "\n",
    "\n",
    "# Convert input strings to BERT encodings\n",
    "train_encodings = tokenizer(X_train, truncation=True, padding=True, max_length=128) \n",
    "test_encodings = tokenizer(X_test, truncation=True, padding=True,  max_length=128)\n",
    "val_encodings = tokenizer(X_val, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert the encodings to Tensorflow objects\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).shuffle(1000).batch(16) \n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val\n",
    ")).batch(64)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jYmT7Yu7c9jm"
   },
   "outputs": [],
   "source": [
    "# Fix bug of transformers package\n",
    "model.roberta.return_dict = False\n",
    "\n",
    "# Compile model\n",
    "callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, \n",
    "                      mode='min', baseline=None, \n",
    "                      restore_best_weights=True)]\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kS0FfKqOdBRM",
    "outputId": "f83d5fa9-d632-4837-a750-311674054958"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "145/145 [==============================] - 71s 489ms/step - loss: 0.9199 - val_loss: 0.9369\n",
      "Epoch 2/10\n",
      "145/145 [==============================] - 67s 464ms/step - loss: 0.9192 - val_loss: 0.9382\n",
      "Epoch 3/10\n",
      "145/145 [==============================] - 67s 461ms/step - loss: 0.9145 - val_loss: 0.9522\n",
      "Epoch 4/10\n",
      "145/145 [==============================] - 67s 463ms/step - loss: 0.9188 - val_loss: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1c011d128>"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, epochs=10, callbacks=callbacks, validation_data=val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS4KKmpGeavD",
    "outputId": "90e1a0dc-d691-4030-941a-881d32646cd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      1.00      0.57       116\n",
      "           1       0.00      0.00      0.00       147\n",
      "           2       0.00      0.00      0.00        25\n",
      "\n",
      "    accuracy                           0.40       288\n",
      "   macro avg       0.13      0.33      0.19       288\n",
      "weighted avg       0.16      0.40      0.23       288\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Check report of the model\n",
    "logits = model.predict(test_dataset)\n",
    "y_preds = np.argmax(logits[0], axis=1)\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFnF6URppjn8"
   },
   "source": [
    "According to the classification report above, the BERTweet model doesn't classify the tweets significantly better than the standard BERT model: on the contrary, the acuraccy f1-score is lower by 8 decimals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfIMyWIlqpK9"
   },
   "source": [
    "#### Look at the confusion matrix of BERTtweets predictions: what can you conclude about the classification of tweets with the label oppositional?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kMA3cj-ofuhT",
    "outputId": "f2e22425-c39b-41df-e072-44ea06b0597a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[116   0   0]\n",
      " [147   0   0]\n",
      " [ 25   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OdN7fQLjqvWY"
   },
   "source": [
    "The algortihm didn't label any tweet as \"oppositional\", not even the ones that actually were (which have been labelled as 'affirmative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5HIyV5k7vNYF"
   },
   "source": [
    "#### During our Q&A yesterday, Tim de Winkel said the interpreting retweets as a communicative act takes some contextual knowledge: you have to know how retweets are generally used. Can you relate this to the performance of the models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that users are likely to retweet tweets they agree with will implicitly create a bias against oppositional retweets, as there will be very few occassions where an user retweets content he does not agree with (some exception would be mocking the opponent, or retweeting old tweets of them where they said something controversial by today's standards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PECxAVxkvfZ9"
   },
   "source": [
    "## Zero shot classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Think of the topics that were relevant during the U.S. elections in the last three months. Use these topics as candidate labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify the first ten tweets in the labeled tweets.p dataset, and evaluate the predicted labels manually. Would this model fruitful to use to label the topics of these tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GHXOtdi0wPB0",
    "outputId": "3b05d155-5912-40a8-a67d-578d6255f39f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartModel: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at facebook/bart-large-mnli were not used when initializing BartForSequenceClassification: ['model.encoder.version', 'model.decoder.version']\n",
      "- This IS expected if you are initializing BartForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "classifier = pipeline(\"zero-shot-classification\")\n",
    "candidate_labels = ['vote', 'election', 'biden', 'trump', 'mail', 'fraud' , 'president', 'ballots']\n",
    "first_ten_tweets = df.text_a[10].to_list()[:10]\n",
    "\n",
    "results = classifier(first_ten_tweets, candidate_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BA6KdftSv9kg",
    "outputId": "065ab43c-8860-4e2e-ce6b-08f56af1ad0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels': ['biden',\n",
       "   'vote',\n",
       "   'president',\n",
       "   'trump',\n",
       "   'election',\n",
       "   'ballots',\n",
       "   'mail',\n",
       "   'fraud'],\n",
       "  'scores': [0.6972934007644653,\n",
       "   0.07789458334445953,\n",
       "   0.05175100639462471,\n",
       "   0.04757499694824219,\n",
       "   0.043722447007894516,\n",
       "   0.03964070603251457,\n",
       "   0.032259780913591385,\n",
       "   0.009863041341304779],\n",
       "  'sequence': \"I was honored to join this ceremony with Legion Post #9 and VFW Post #1617 in Derry to honor Gold Star Mother Nancy Geary.\\n\\nWe will always remember the sacrifice of her son, LCpl. Michael Geary, as well as Nancy's sacrifice, courage, and resiliency.\\n\\nhttps://t.co/Yl0Bm4wkyD\"},\n",
       " {'labels': ['biden',\n",
       "   'vote',\n",
       "   'mail',\n",
       "   'president',\n",
       "   'trump',\n",
       "   'ballots',\n",
       "   'election',\n",
       "   'fraud'],\n",
       "  'scores': [0.5052504539489746,\n",
       "   0.17508500814437866,\n",
       "   0.08642823994159698,\n",
       "   0.06787031143903732,\n",
       "   0.06729519367218018,\n",
       "   0.05534692108631134,\n",
       "   0.030514873564243317,\n",
       "   0.012209057807922363],\n",
       "  'sequence': \"RT @nextstepsidaho: @SenatorRisch @IdSBOE Thanks for the shout out, Senator! \\n\\nThe fair is open T, W, &amp; Th from 8 AM-8 PM (MDT)\\n👀 https://t.co/ofoxecUBN3\\n\\n➡️ 11 #Idaho colleges \\n➡️ #finAid with the @IdSBOE #Scholarship Mgr\\n➡️ #prizes\\n\\nIt's an easy way for #Idahoans to explore #continuingEducation close to #home.\"},\n",
       " {'labels': ['election',\n",
       "   'vote',\n",
       "   'biden',\n",
       "   'president',\n",
       "   'ballots',\n",
       "   'fraud',\n",
       "   'mail',\n",
       "   'trump'],\n",
       "  'scores': [0.4094786047935486,\n",
       "   0.3614402711391449,\n",
       "   0.13344140350818634,\n",
       "   0.033925119787454605,\n",
       "   0.02861757203936577,\n",
       "   0.01404853817075491,\n",
       "   0.00998953077942133,\n",
       "   0.009058959782123566],\n",
       "  'sequence': \"On Sunday, Bolivia will vote to determine its future. These elections must be free and fair and reflect the will of the people. The Áñez administration must respect the electoral process, freedom of the press, and ensure no one interferes with or attempts to intimidate voters. https://t.co/IoFX4bPruK QUOTE @ceprdc: Results in #Bolivia's elections on Sunday, October 18, may be less transparent than last year's. Here's why. #BoliviaElections #BoliviaCoup https://t.co/ejj1Ny7fFK\"},\n",
       " {'labels': ['biden',\n",
       "   'vote',\n",
       "   'election',\n",
       "   'ballots',\n",
       "   'president',\n",
       "   'mail',\n",
       "   'fraud',\n",
       "   'trump'],\n",
       "  'scores': [0.6079854369163513,\n",
       "   0.11921261250972748,\n",
       "   0.06140796095132828,\n",
       "   0.04924304783344269,\n",
       "   0.047567639499902725,\n",
       "   0.04651878401637077,\n",
       "   0.043949659913778305,\n",
       "   0.02411475218832493],\n",
       "  'sequence': 'We have two paths before us: \\n\\nRepublican: \\n\\n✅ law &amp; order \\n✅ economic growth \\n✅ boundless patriotism \\n\\nor\\n\\nDemocrat: \\n\\n❌ rioting in streets \\n❌ defunding frontline police \\n❌ bankrupting middle class w/ socialism \\n\\nRead my #CommitmentToAmerica oped👇🏻\\nhttps://t.co/EUEHJh1l54'},\n",
       " {'labels': ['vote',\n",
       "   'biden',\n",
       "   'mail',\n",
       "   'election',\n",
       "   'ballots',\n",
       "   'president',\n",
       "   'trump',\n",
       "   'fraud'],\n",
       "  'scores': [0.4304948151111603,\n",
       "   0.3708530366420746,\n",
       "   0.05597694218158722,\n",
       "   0.04986471310257912,\n",
       "   0.04085991531610489,\n",
       "   0.023639364168047905,\n",
       "   0.02224273607134819,\n",
       "   0.00606854073703289],\n",
       "  'sequence': \"Tomorrow marks 100 years since the ratification of the 19th Amendment! Securing this essential right for women changed the course of American history, &amp; I'm glad to celebrate this significant milestone alongside the women of our country. Read more from my Weekly Column via @yhn: https://t.co/EVUkZTGsNi QUOTE @yhn: @RepMarthaRoby: 'Securing the right to vote for women changed the course of American history' https://t.co/eLmJI3dbnc https://t.co/bFx4SgA0Mg\"},\n",
       " {'labels': ['biden',\n",
       "   'vote',\n",
       "   'mail',\n",
       "   'ballots',\n",
       "   'election',\n",
       "   'president',\n",
       "   'trump',\n",
       "   'fraud'],\n",
       "  'scores': [0.5884072184562683,\n",
       "   0.13124975562095642,\n",
       "   0.05869518592953682,\n",
       "   0.051703255623579025,\n",
       "   0.051695723086595535,\n",
       "   0.043671101331710815,\n",
       "   0.0410371832549572,\n",
       "   0.033540599048137665],\n",
       "  'sequence': \"RT @HouseGOP: It's time to get serious about holding China accountable.\\n \\nThe China Task Force’s blueprint reverses the failed consensus on the CCP and responds to urgent threats to our safety, security, and self-sufficiency.\\n\\nNEW from @GOPLeader and @RepMcCaul\\n \\n↓ ↓\\nhttps://t.co/Y3OwGHiWLq\"},\n",
       " {'labels': ['biden',\n",
       "   'vote',\n",
       "   'mail',\n",
       "   'trump',\n",
       "   'ballots',\n",
       "   'fraud',\n",
       "   'election',\n",
       "   'president'],\n",
       "  'scores': [0.6482546329498291,\n",
       "   0.13128595054149628,\n",
       "   0.04710141569375992,\n",
       "   0.04328945651650429,\n",
       "   0.04165761172771454,\n",
       "   0.03036806918680668,\n",
       "   0.030186260119080544,\n",
       "   0.027856623753905296],\n",
       "  'sequence': 'RT @ksatnews: Analysis: COVID-19’s almost incomprehensible toll in Texas https://t.co/m4sLm73qlc'},\n",
       " {'labels': ['biden',\n",
       "   'president',\n",
       "   'vote',\n",
       "   'fraud',\n",
       "   'mail',\n",
       "   'trump',\n",
       "   'election',\n",
       "   'ballots'],\n",
       "  'scores': [0.6736680269241333,\n",
       "   0.089470773935318,\n",
       "   0.07542566955089569,\n",
       "   0.05531066656112671,\n",
       "   0.03427644819021225,\n",
       "   0.03164132684469223,\n",
       "   0.020917294546961784,\n",
       "   0.019289808347821236],\n",
       "  'sequence': 'This administration continues to politicize science and stifle research findings. \\n\\nD or R, we should all be able to agree that this is completely unacceptable. \\n\\nhttps://t.co/ZXxsZnKIL9'},\n",
       " {'labels': ['biden',\n",
       "   'election',\n",
       "   'vote',\n",
       "   'fraud',\n",
       "   'mail',\n",
       "   'president',\n",
       "   'ballots',\n",
       "   'trump'],\n",
       "  'scores': [0.5076772570610046,\n",
       "   0.3755766451358795,\n",
       "   0.0413813441991806,\n",
       "   0.01913939230144024,\n",
       "   0.015802958980202675,\n",
       "   0.014652157202363014,\n",
       "   0.014401422813534737,\n",
       "   0.011368812993168831],\n",
       "  'sequence': 'No more business as usual. Even if tools &amp; time are limited, we must take a stand against Republicans’ rush to break norms &amp; shatter rules. Raw political power should not install an activist Justice days before an election. https://t.co/jdkfM9EpXE'},\n",
       " {'labels': ['biden',\n",
       "   'vote',\n",
       "   'mail',\n",
       "   'fraud',\n",
       "   'ballots',\n",
       "   'trump',\n",
       "   'election',\n",
       "   'president'],\n",
       "  'scores': [0.6793232560157776,\n",
       "   0.07160636782646179,\n",
       "   0.06860242784023285,\n",
       "   0.04979297146201134,\n",
       "   0.038957010954618454,\n",
       "   0.03574562817811966,\n",
       "   0.034652695059776306,\n",
       "   0.021319646388292313],\n",
       "  'sequence': 'Investigation after investigation, news report after news report, has shown the cruel treatment of immigrants in detention centers has put their wellbeing and lives at risk.\\n\\nWe can’t allow one more death. ICE must start releasing immigrants. https://t.co/fSSZfpFoj7 QUOTE @CNNPolitics: Immigrants in US custody died after \"inadequate\" medical care, congressional investigation finds https://t.co/DuFAjf0oWP https://t.co/aG5TThg2uH'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzZCgsALBIKj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 2.1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
