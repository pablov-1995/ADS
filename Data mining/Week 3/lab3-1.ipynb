{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "# pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Joris correction of the year of each song\n",
    "\n",
    "PATH_DF = 'english_cleaned_lyrics.csv'\n",
    "PATH_CORRECTION = 'indx2newdate.p'\n",
    "\n",
    "def load_dataset(data_path, path_correction):\n",
    "    df = pd.read_csv(data_path)\n",
    "    indx2newdate = pickle.load(open(PATH_CORRECTION, 'rb'))\n",
    "    df['year'] = df['index'].apply(lambda x: int(indx2newdate[x][0][:4]) if indx2newdate[x][0] != '' else 0)\n",
    "    return df[df.year > 1960][['song', 'year', 'artist', 'genre', 'lyrics']]\n",
    "    \n",
    "dataset = load_dataset(PATH_DF, PATH_CORRECTION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Count how many songs of each genre are in the data set, and pick a genre that 1) you think is interesting to explore and 2) has over 5.000 songs. Make a subset of the data set only containing songs of that genre; this is the data set you work with for the rest of these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Folk           1373\n",
       "R&B            2338\n",
       "Other          2449\n",
       "Indie          2489\n",
       "Jazz           5068\n",
       "Electronic     5194\n",
       "Country       10545\n",
       "Hip-Hop       14878\n",
       "Metal         15671\n",
       "Pop           23295\n",
       "Rock          77556\n",
       "Name: song, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.groupby(by=\"genre\").count()[\"song\"].sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will choose Hip-Hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiphop = dataset[dataset.genre == \"Hip-Hop\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Inspect the number of songs for each year, either using a data frame or using a visualization. Do you think you have enough songs for each year (at least more than fifty)? If not, filter out the years that do not contain enough songs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1961, 1963, 1966, 1969, 1970, 1971, 1974, 1977, 1978, 1979, 1980,\n",
       "            1981, 1982, 1983, 1984, 1985, 1986, 1987],\n",
       "           dtype='int64', name='year')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = hiphop.groupby(by=\"year\").count()[\"song\"].apply(lambda v: v if v <= 50 else None).dropna().index\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't any year where the number of songs is lower or equal than 50. So we will keep all years for our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Process the texts of your genre (and only your genre!) using Spacy. Extract the lemmatized tokens for each song, and remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the texts\n",
    "data = hiphop[~hiphop.year.isin(ids)].sample(n=5000).lyrics.values  # taking a small sample because my computer crashes with a bigger one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [06:08<00:00, 13.55it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "processed_text = [nlp(text.lower()) for text in tqdm(data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "lemma = [[token.lemma_ for token in text if not token.is_punct and not token.is_stop] for text in processed_text]\n",
    "with open('lemma.json', 'w') as file:\n",
    "    json.dump(lemma, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('lemma.json') as json_file:\n",
    "    lemma = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create a dictionary and filter out the words that occur less than three times, and all words that occur in over 85% of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "MIN_DF = 3 # minium document frequency\n",
    "MAX_DF = 0.85 # maximum document frequency\n",
    "dictionary = Dictionary(lemma) # get the vocabulary\n",
    "dictionary.filter_extremes(no_below=MIN_DF, \n",
    "                           no_above=MAX_DF)\n",
    "corpus = [dictionary.doc2bow(text) for text in lemma]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train a topic model with 50 topics and inspect the output, both using the ten most relevant words for each topics, and using pyLDAvis. Now also run a topic model with 20 topics, and one with 100 topics. Be sure to save the models using lda.save(’folder/to/save’) What number of topics does result in the “best” topics? [Note: how you operationalize “best” is up to you]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (50 topics)\n",
    "\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "PATH_TO_MALLET = '/home/pablo/Documents/github-repos/Mallet/bin/mallet'\n",
    "N_TOPICS = 50\n",
    "N_ITERATIONS = 1000\n",
    "\n",
    "lda = LdaMallet(PATH_TO_MALLET,\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=N_TOPICS,\n",
    "                iterations=N_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save('fifty_topics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (20 topics)\n",
    "\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "PATH_TO_MALLET = '/home/pablo/Documents/github-repos/Mallet/bin/mallet'\n",
    "N_TOPICS = 20\n",
    "N_ITERATIONS = 1000\n",
    "\n",
    "lda = LdaMallet(PATH_TO_MALLET,\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=N_TOPICS,\n",
    "                iterations=N_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save('twenty_topics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (20 topics)\n",
    "\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "PATH_TO_MALLET = '/home/pablo/Documents/github-repos/Mallet/bin/mallet'\n",
    "N_TOPICS = 100\n",
    "N_ITERATIONS = 1000\n",
    "\n",
    "lda = LdaMallet(PATH_TO_MALLET,\n",
    "                corpus=corpus,\n",
    "                id2word=dictionary,\n",
    "                num_topics=N_TOPICS,\n",
    "                iterations=N_ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.save('hundred_topics.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models (I added this so I don't have to train them all the time)\n",
    "\n",
    "lda_twenty = LdaMallet.load('twenty_topics.model')\n",
    "lda_fifty = LdaMallet.load('fifty_topics.model')\n",
    "lda_hundred = LdaMallet.load('hundred_topics.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide which models performs best, I will use the coherence score of each of them. This index measures how coherent the topics are with the words of our dataset. The closest it is to 1, the better the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/anaconda3/envs/ADS/lib/python3.7/site-packages/gensim/models/ldamodel.py:1108: RuntimeWarning: invalid value encountered in multiply\n",
      "  score += np.sum((self.eta - _lambda) * Elogbeta)\n",
      "/home/pablo/anaconda3/envs/ADS/lib/python3.7/site-packages/gensim/models/ldamodel.py:1109: RuntimeWarning: invalid value encountered in subtract\n",
      "  score += np.sum(gammaln(_lambda) - gammaln(self.eta))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.4779881143808568\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_hundred, texts=lemma, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  nan\n",
      "\n",
      "Coherence Score:  0.4480110762777182\n"
     ]
    }
   ],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_fifty, texts=lemma, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity:  nan\n",
      "\n",
      "Coherence Score:  0.42100125806247934\n"
     ]
    }
   ],
   "source": [
    "# Compute Perplexity\n",
    "lda_conv = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(lda_twenty)\n",
    "print('\\nPerplexity: ', lda_conv.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_twenty, texts=lemma, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hundred topics seems to work best as it has the highest coherence score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
